{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Feature Analysis & Engineering\n",
    "## Deep Dive into Customer Behavior Patterns\n",
    "\n",
    "### Feature Importance Analysis\n",
    "\"\"\"\n",
    "\n",
    "# Import modules\n",
    "from feature_engineering import FeatureEngineer\n",
    "from statistical_tests import StatisticalAnalyzer\n",
    "from visualization import AdvancedVisualizer\n",
    "\n",
    "# Load cleaned data\n",
    "df = pd.read_pickle('data/processed/cleaned_data.pkl')\n",
    "\n",
    "print(\"ðŸ”§ Starting feature engineering pipeline...\")\n",
    "\n",
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer(df)\n",
    "\n",
    "# Step 1: Create temporal features\n",
    "print(\"\\\\n1. Creating temporal features...\")\n",
    "engineer.create_temporal_features()\n",
    "\n",
    "# Step 2: Create behavioral features  \n",
    "print(\"2. Creating behavioral features...\")\n",
    "engineer.create_behavioral_features()\n",
    "\n",
    "# Step 3: Create interaction features\n",
    "print(\"3. Creating interaction features...\")\n",
    "engineer.create_interaction_features()\n",
    "\n",
    "# Step 4: Encode categorical variables\n",
    "print(\"4. Encoding categorical features...\")\n",
    "engineer.encode_categorical_features()\n",
    "\n",
    "# Step 5: Feature selection\n",
    "print(\"5. Performing feature selection...\")\n",
    "final_df, feature_scores = engineer.select_best_features(target_column='churn', k=15)\n",
    "\n",
    "print(\"\\\\nðŸŽ¯ Top 10 Most Predictive Features:\")\n",
    "display(feature_scores.head(10))\n",
    "\n",
    "\"\"\"\n",
    "### Statistical Validation of Features\n",
    "\"\"\"\n",
    "\n",
    "# Perform statistical tests\n",
    "analyzer = StatisticalAnalyzer(final_df)\n",
    "stat_report = analyzer.generate_statistical_report()\n",
    "\n",
    "print(\"\\\\nðŸ“Š Statistical Significance Summary:\")\n",
    "significant_tests = [t for t in stat_report['hypothesis_tests'] if t['significant']]\n",
    "for test in significant_tests[:5]:\n",
    "    print(f\"âœ… {test['test']}: p-value = {test['p_value']:.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "### Feature Distribution Analysis\n",
    "\"\"\"\n",
    "\n",
    "# Analyze distributions of top features\n",
    "top_features = feature_scores.head(6)['feature'].tolist()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    if feature in final_df.columns:\n",
    "        # Plot distribution by churn status\n",
    "        final_df[final_df['churn'] == 0][feature].hist(ax=axes[i], alpha=0.7, \n",
    "                                                      label='Retained', bins=20)\n",
    "        final_df[final_df['churn'] == 1][feature].hist(ax=axes[i], alpha=0.7, \n",
    "                                                      label='Churned', bins=20)\n",
    "        axes[i].set_title(f'{feature}\\\\nDistribution by Churn')\n",
    "        axes[i].legend()\n",
    "        axes[i].set_xlabel(feature.replace('_', ' ').title())\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "### Correlation Network Analysis\n",
    "\"\"\"\n",
    "\n",
    "# Create correlation network for top features\n",
    "import networkx as nx\n",
    "\n",
    "top_features_network = top_features + ['churn']\n",
    "corr_network = final_df[top_features_network].corr()\n",
    "\n",
    "# Create graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges\n",
    "for i, feature1 in enumerate(top_features_network):\n",
    "    for j, feature2 in enumerate(top_features_network):\n",
    "        if i < j and abs(corr_network.iloc[i, j]) > 0.3:\n",
    "            G.add_edge(feature1, feature2, weight=abs(corr_network.iloc[i, j]))\n",
    "\n",
    "# Plot network\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "# Node colors based on correlation with churn\n",
    "node_colors = [corr_network.loc[node, 'churn'] for node in G.nodes()]\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                      node_size=800, cmap='coolwarm', \n",
    "                      vmin=-1, vmax=1)\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.5, \n",
    "                      width=[G[u][v]['weight'] * 3 for u, v in G.edges()])\n",
    "nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "\n",
    "plt.title('Feature Correlation Network\\\\nRelationship Strength Visualization')\n",
    "plt.colorbar(plt.cm.ScalarMappable(cmap='coolwarm', \n",
    "                                  norm=plt.Normalize(-1, 1)), \n",
    "             label='Correlation with Churn')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\nðŸ’¡ Feature analysis completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
